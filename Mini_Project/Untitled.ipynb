{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c999ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow, show, subplot, figure, axis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "d79007ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['testd', 'testx', 'traind', 'trainx', 'vald', 'valx', 'words']>\n"
     ]
    }
   ],
   "source": [
    "filename = \"data2.h5\"\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    y_tets = f[list(f.keys())[0]][:]\n",
    "    x_test = f[list(f.keys())[1]][:]\n",
    "    y_train = f[list(f.keys())[2]][:]\n",
    "    x_train = f[list(f.keys())[3]][:]\n",
    "    y_val = f[list(f.keys())[4]][:]\n",
    "    x_val = f[list(f.keys())[5]][:]\n",
    "    words = f[list(f.keys())[6]][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "dba6bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(x, maxInd = 250):\n",
    "    out = np.zeros(maxInd)\n",
    "    out[x-1] = 1\n",
    "    return out\n",
    "\n",
    "def input_vector(data):\n",
    "    encoded_data = []\n",
    "    for row in data:\n",
    "        word_1 = word_vector(row[0])\n",
    "        word_2 = word_vector(row[1])\n",
    "        word_3 = word_vector(row[2])\n",
    "        row = np.concatenate((word_1,word_2,word_3))\n",
    "        row = row.reshape(1,len(row))\n",
    "        encoded_data.append(row)\n",
    "    return encoded_data\n",
    "\n",
    "def output_vector(data):\n",
    "    encoded_data = []\n",
    "    for row in data:\n",
    "        word = word_vector(row)\n",
    "        encoded_data.append(word)\n",
    "    return encoded_data\n",
    "\n",
    "def initialize_weights(D,P,data,mean=0,std=0.01):\n",
    "    N = 200\n",
    "    W0 = np.random.normal(mean,std, 750*D).reshape(750, D)\n",
    "    W1 = np.random.normal(mean,std, D*P).reshape(D,P)\n",
    "    W2 = np.random.normal(mean,std, P*250).reshape(P,250)\n",
    "    b1 = np.random.normal(mean,std, N*P).reshape(N,P)\n",
    "    b2 = np.random.normal(mean,std, N*250).reshape(N,250)\n",
    "    \n",
    "    we = [W0,W1,W2,b1,b2]\n",
    "    \n",
    "    return we\n",
    "\n",
    "def sigmoid(x):\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "def sigmoid_backward(x):\n",
    "    d_sig = x*(1-x)\n",
    "    return d_sig\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - np.max(x))\n",
    "    y = expx/np.sum(expx, axis=0)\n",
    "    return y\n",
    "\n",
    "def cross_entrophy(y,y_pred):\n",
    "    return (np.sum(- y * np.log(y_pred))/ y.shape[0])\n",
    "\n",
    "def forward(data, we):\n",
    "    \n",
    "    w0,w1,w2,b1,b2 = we\n",
    "\n",
    "    z0 = np.dot(data,w0) #first layer linear forward\n",
    "    A0 = z0 #first layer without activation\n",
    "    z1 = np.dot(A0,w1) + b1 #second layer linear forward\n",
    "    A1 = sigmoid(z1) #second layer activation\n",
    "    z2 =  np.dot(A1,w2) + b2 #output linear forward\n",
    "    output = softmax(z2) #output layer activation\n",
    "\n",
    "    return A0,A1,output\n",
    "\n",
    "def calculate_cost(data,y,we):\n",
    "    N = data.shape[0]\n",
    "    W0,W1,W2,b1,b2 = we\n",
    "    A0,A1,output = forward(data, we)\n",
    "    d_sig = sigmoid_backward(A1)\n",
    "    #calculate cost\n",
    "    cost = cross_entrophy(y,output)\n",
    "    #calculate gradients\n",
    "    d_w0 = np.dot(data.T,((np.dot(((np.dot((y-output),W2.T))*d_sig),W1.T))*A0))\n",
    "    d_w1 = np.dot(A0.T,(np.dot((y-output),W2.T)*d_sig))\n",
    "    d_w2 = np.dot(A1.T,(y-output))\n",
    "    d_b1 = np.dot((y-output),W2.T)*d_sig\n",
    "    d_b2 = y-output\n",
    "    \n",
    "    grads = [d_w0,d_w1,d_w2,d_b1,d_b2]\n",
    "    \n",
    "    return cost, grads\n",
    "\n",
    "def backward(data,y,lr_rate,momentum, we, old_grads):\n",
    "    #get gradients\n",
    "    cost, grads = calculate_cost(data,y,we)\n",
    "    #update weights \n",
    "    we[0] -= lr_rate*grads[0]+old_grads[0]*momentum\n",
    "    we[1] -= lr_rate*grads[1]+old_grads[1]*momentum\n",
    "    we[2] -= lr_rate*grads[2]+old_grads[2]*momentum\n",
    "    we[3] -= lr_rate*grads[3]+old_grads[3]*momentum\n",
    "    we[4] -= lr_rate*grads[4]+old_grads[4]*momentum\n",
    "\n",
    "    return cost, grads, we\n",
    "    \n",
    "def train(x_train,y_train,D,P,epoch,num_batch,lr_rate,momentum):\n",
    "    costs = []\n",
    "    epochs = []\n",
    "    data = input_vector(x_train)\n",
    "    data = np.squeeze(data,axis=1)\n",
    "    label = output_vector(y_train)\n",
    "    label = np.array(label)\n",
    "    we = initialize_weights(D,P,data,mean=0,std=0.01)\n",
    "    momentum = 0.0000085\n",
    "    cost, old_grads = calculate_cost(data[:200],label[0:200],we)\n",
    "    lr_rate = 0.0000015\n",
    "    for i in range (epoch):\n",
    "        epochs.append(i)\n",
    "        for j in range (num_batch):\n",
    "            data_batch = data[200*j:200*j+200]\n",
    "            label_batch = label[200*j:200*j+200]\n",
    "            cost, grads,we = backward(data_batch,label_batch, lr_rate,momentum,we,old_grads)\n",
    "        costs.append(cost)\n",
    "    j = 0\n",
    "    for i in reversed (costs):\n",
    "        print(\"Epoch: {} --------------> Loss: {} \".format(j+1,i))\n",
    "        j+=1\n",
    "    return we\n",
    "\n",
    "def random_index(sample_size):\n",
    "    random_index = []\n",
    "    for i in range(sample_size):\n",
    "        index = random.randint(0,46500)\n",
    "        random_index.append(index)\n",
    "    return random_index\n",
    "\n",
    "def pick_sample(data,label,sample_size):\n",
    "    sample = []\n",
    "    labels = []\n",
    "    sample_index = random_index(sample_size)\n",
    "    for i in sample_index:\n",
    "        sample.append(data[i])\n",
    "        labels.append(label[i])\n",
    "    return sample,labels\n",
    "\n",
    "def predict(words,output):\n",
    "    pred_rows = []\n",
    "    for i in range(len(output)):\n",
    "        word_index = output[i].argsort()[-10:][::-1]\n",
    "        pred_words = []\n",
    "        for word in word_index: \n",
    "            pred_words.append(str(words[word].decode(\"utf-8\")))\n",
    "        pred_rows.append((pred_words))\n",
    "    return pred_rows\n",
    "\n",
    "def print_preds(random_sample,test_label,pred_rows,words):\n",
    "    for i in range(len(random_sample)):\n",
    "        tri = \"sample trigram: \"\n",
    "        for j in range(len(random_sample[i])):\n",
    "            tri+=str(words[random_sample[i][j]].decode(\"utf-8\"))+\" \"\n",
    "        tri += \" ----> label: \" + str(words[test_label[i]].decode(\"utf-8\"))\n",
    "        print(tri)\n",
    "        print(\"Top 10 predictions: \",pred_rows[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "fd7492c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 --------------> Loss: 5.300577973033711 \n",
      "Epoch: 2 --------------> Loss: 5.300131218169797 \n",
      "Epoch: 3 --------------> Loss: 5.299716461160852 \n",
      "Epoch: 4 --------------> Loss: 5.299331194717817 \n",
      "Epoch: 5 --------------> Loss: 5.298973076902554 \n",
      "Epoch: 6 --------------> Loss: 5.298639896318873 \n",
      "Epoch: 7 --------------> Loss: 5.298329542234294 \n",
      "Epoch: 8 --------------> Loss: 5.298039979388973 \n",
      "Epoch: 9 --------------> Loss: 5.297769227289148 \n",
      "Epoch: 10 --------------> Loss: 5.297515343809774 \n"
     ]
    }
   ],
   "source": [
    "we = train(x_train,y_train,32,256,10,1000,0.15,0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "4ed60f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample,test_label = pick_sample(x_test,y_test,200)\n",
    "random_sample_vector = input_vector(random_sample)\n",
    "random_sample_vector = np.squeeze(random_sample_vector,axis=1)\n",
    "random_sample = random_sample[:5]\n",
    "test_label  = test_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "1304d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,output = forward(random_sample_vector, we)\n",
    "output = output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "d5f625fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample trigram: members it him  ----> label: left\n",
      "Top 10 predictions:  ['here', 'say', 'people', 'should', 'not', 'before', 'can', 'big', 'there', 'way']\n",
      "sample trigram: well them day  ----> label: then\n",
      "Top 10 predictions:  ['.', 'to', 'know', 'make', '?', 'before', 'at', 'nt', 'who', 'can']\n",
      "sample trigram: including said it  ----> label: under\n",
      "Top 10 predictions:  ['.', '?', 'money', 'never', 'know', 'right', 'all', 'work', 'director', 'life']\n",
      "sample trigram: night today even  ----> label: political\n",
      "Top 10 predictions:  ['.', '?', 'over', 'see', ',', 'since', 'office', 'your', 'old', 'percent']\n",
      "sample trigram: though world set  ----> label: their\n",
      "Top 10 predictions:  ['them', 'will', 'dr.', 'world', 'part', 'states', 'as', 'what', 'home', 'other']\n"
     ]
    }
   ],
   "source": [
    "pred_rows = predict(words,output)\n",
    "print_preds(random_sample,test_label,pred_rows,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a65fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94319fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7493e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe83d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f749537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
